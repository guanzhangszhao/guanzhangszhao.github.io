---
layout: post
title: Blog Post 2 Spectral Clustering
---

This is a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. We will break the algorithm into different pieces and approach the problem step by step.

### Notation

In all the math below: 

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors (1d arrays of numbers). 
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (`A@B`). $$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (`A@v`). 
- **Most of the mathematical discussion involving formulas and equations are adapted from PIC16B Assignment Prompt Notebook by by Professor Chodrow at UCLA**


## Motivation

When we are dealing with datasets that contain multiple groups of data points, we would always want to ask the question: how can we classify these data points into different group? This is also known as *clustering*. The most common techniques to do so is through using the K-Means algorithm. We have some simple realization below.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

First, let's generate a random dataset we can feed to the K-means algorithm.
```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```


![jpg](/images
/2/0_1.jpg)


K-Means algorithm requires us to tell it how many clusters we want to have in the result. And then, it will try to find these two clusters and allocate each points to the cluster that's closest to the data points itself. Here, being close to the cluster means there's a relatively short distance between the point and the center (mean) of the cluster.

If we apply the K-means algorithm to the dataset we generate above, it identifies the correct clusters!

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```



![jpg](/images
/2/0_2.jpg)




### Harder Clustering

Intuitively, we expect the K-means algorithm to function very well on clusters that have a circular shpae just like the ones above. However, what if the dataset are not shaped that way? What if we supply it with crescents instead of blobs?


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```



![jpg](/images
/2/0_3.jpg)

Well, K-Means algorithm doesn't seem to work well on this particular dataset!


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```


![jpg](/images
/2/0_4.jpg)



To deal with this mistake here, we want to find some other algorithm that can correctly identify the *crescents*. And as we will see in the following parts, *spectral clustering* provides a correct solution. We are going to construct this new algorithm step by step.



## An Initial Construction of Spectral Clustering

Note the *Initial* in the titel of this section. This means, in this section, we are building everything from scratch: we are *manually* constructing a loss function, we are *manually* minimizing it and we are going to get a correct result! However, this process is tedious, and in later parts of this blog, we are going to see a purely **algebraic solution** to the problem, where we will be doing the same thing but using linear algebra instead. That's going to expedite the clustering process as much as possible!

### Similarity Matrix

The key to any clustering problem is to define **similarity**. That is to say, we need to let the algorithm know, which points are similar. In the K-Means algorithm, we find two cluster centers and look at each individual points to find out they are closer to which of the two centers. But in *spectral clustering*, we will first look at the *similarity* between individual points.

We define similarity in the following way- if two points are within `epsilon` distance from each other, then they should be *similar* to each other. And we will construct an aggregate matrix to store all of these similarity relationship between points. This matrix is known as the *similarity matrix*, we denote it as $$\mathbf{A}$$.

Intuitively, for a list of `n` points, $$\mathbf{A}$$ should be a matrix (2d `np.ndarray`) with shape `(n, n)`. If point `i,j` are *similar* with each other (i.e. $$d(i,j) < \epsilon$$), then we write entry `A[i,j]` to be `1`, and zero otherwise.

**Note: The diagonal entries `A[i,i]` should all be equal to zero because we don't consider points to be similar to themselves**  

For this part, we will use `epsilon = 0.16`. At the end of this blog post, we will be looking at the impact of different epsilons on the clustering.

{::options parse_block_html="true" /}
<div class="gave-help">
Originally, we have exprimented with `epsilon = 0.4` and some classmates of mine have got unsatisfactory clustering results on the dataset we generated above.

Based on my own experience, I used `epsilon = 0.16` instead and this will help us get through the clustering successfully. I've given advices to some of classmates, suggesting that they might want to manipulate this as well if they encounter `weird` results.

For the readers, we don't need to worry about the precise threshold we are using here, and we will discuss the influence of these different epsilons at the end of the blog post.
</div>
{::options parse_block_html="false" /}


```python
# We use a standardized pairwise_distances method from the sklearn.metrics package
from sklearn.metrics import pairwise_distances
# The function will calculate the pairwise distance of a vector with a specified method
# Here, we use euclidean, which is the standard L2 norm
dist = pairwise_distances(X, metric='euclidean')
# As mentioned above, we set epsilon = 0.4
epsilon = 0.16
# We check if an element of the distance matrix is smaller than epsilon**2
# And we simply recode 1 for True and 0 for False
A = (dist < epsilon).astype(int)
# Fill the diagonal with 0
np.fill_diagonal(A,0)
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           ...,
           [0, 0, 0, ..., 0, 0, 1],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 1, 0, 0]])



### Loss Function


Once the algorithm knows what points are *similar*, the next intuitive move would be to put the similar points into a cluster! In other words, if two points are similar, they'd better be in the same cluster, and we want reduce the *similarity relationship cross clusters* to its minimal. This is natural idea behind the loss function we are going to construct.

However, you might immediately realize we can `trick` the loss function very easily- what if we pick one point at the corner of one crescent to be a cluster and the rest to be another cluster. Because there's just one point in the first cluster, we would never see a lot of *similarity relationship cross clusters*! And to also encapsulte this idea, we will also take into our construction of the loss function.

Let's construct the loss function!


#### The Cut Term

*Similarity relationship cross clusters* is a very wordy expression. However, this is simple mathematically! We give it a name **cut**: $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the **cut** of the clusters $$C_0$$ and $$C_1$$. 

Empirically speaking, this mathematical sum is nothing but the total number of similarity relationship between points `a,b` such that `a,b` belong to different clusters. In other words, for all pairs of `a,b` in the datasets: 1) If they belong to the same cluster, we do nothing; 2) If they belong to different clusters but are not similar, we do nothing either; 3) If they belong to different clusters and they are **similar**, we add **1** to the **cut** term.

In Python language, the **cut** function can be easily defined in the following way, where $$mathbf{A}$$ is the similarity matrix, and **y** is the vector that contains the label for each point (`y[i] = 0` means the i-th point belongs to $$C_0$$ and `y[i] = 1` means the i-th point belongs to $$C_1$$ ).

```python
def cut(A,y):
    # We use Matrix Multiplication to Accelerate the Process
    # First, reshape y to be two-dimensional
    b = y.reshape((1,-1))
    # Note that y \cdot y^{T} yields a connection matrix for points in cluster 1
    # Likewise, (y-1) \cdot (y-1)^{T} yields a connection matrix for points in cluster 0
    # Adding these two matrix up and the remaining 0 elements in the matrix are connections
    # across clusters and we now mark these elements as 1 and others 0
    B = np.absolute(b * b.T +(b-1)*(b-1).T - 1)
    # When we multiply A with B elements wise, all similarity within clusters are sent to zero
    # And the remaining non-zero elements (1) are paris of points which are both similar and
    # from different cluster. We return the sum of the matrix
    return (A*B).sum()
```

If we compute the cut objective for the true clusters `y`, we can see that the result is essentially zero because we are doing the right clustering and no association is established accross clusters. 


```python
# Run the cut function on the true labels
cut(A,y)
```




    0


However, if we generate a random vector of random labels of length `n`, with each label equal to either 0 or 1, the result of the cut function is non-zero and substantially large. 

```python
# Run the cun function on random labels
ra = np.random.choice(2,200)
cut(A,ra)
# Indeed much larger than the result of running cut on true lables
```




    740


This shows that this part of the cut objective indeed favors the true clusters over the random ones. 


#### The Volume Term 

Now we deal with the second problem we discussed above- we want each clusters to be of moderate size so that the loss function will not be tricked by the solution we talked aboout above. Thus we also define a *volume term* to handle the problem: $$\mathbf{vol}(C_0) := \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). 



Therefore, we will write a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple. For example, `v0, v1 = vols(A,y)` should result in `v0` holding the volume of cluster `0` and `v1` holding the volume of cluster `1`. Then, write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. This will allow us to build towards a normcut function which we can minize in clustering.

```python
def vols(A,y):
    # Calculate v0 using only i-th rows where point i is in cluster 0 
    v0 = A[y == 0,:].sum()
    # Calculate v1 using only i-th rows where points i is in cluster 1
    v1 = A[y == 1,:].sum()
    # Return the result
    return (v0, v1)
```


#### The Loss Function

Finally, we are able to define the loss function for our clustering problem. Note that by the definition of the loss function, the loss should be large when: 

- 1) There are two many similar points separated into different cluster;
- 2) The size of one cluster is relatively small.

Thus, we briefly manipulate the two functions we definied above and let the final loss function be the following, to which we give the name **normcut()**:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$


In Python, this **normcut** function can be constructed as following:

```python
def normcut(A,y):
    # Get v0 and v1 using vols function
    v0, v1 = vols(A,y)
    # Return the normcut value using the fomula
    return cut(A,y)*(1/v0 + 1/v1)
```

Now, compare the `normcut` objective using both the true labels `y` and the fake labels. As expected, when dealing with the true label, the normcut function returns 0 as the cut term by itself, as we calculated above, has already been zero. Thus, the normcut result will always be zero.



```python
# This is effectively zero given cut(A,y) is 0
normcut(A,y)
```




    0.0

{::options parse_block_html="true" /}
<div class="got-help">
Originally, I simply stated that the normcut on the correct labels should be zero but didn't discuss why this should be the case. One of my classmate pointed out that including a brief discussion would be helpful.
</div>
{::options parse_block_html="false" /}



However, this is not true when we apply normcut to the randomly generated labels above. We see the result is larger than 0.

```python
# Run normcut on random labels and we get a larger result
normcut(A,ra)
```




    1.9638846652810131

#### Final Touch on the Loss Function

We can of course now try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We will transform the problem a bit using some math tools. *This is not the faster version we promised above. We are going to implement that later* 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

Next, using linear algebra, we can show that

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;.$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before. And we know that using linear algebra can facilitate the process a lot.

To show the two approaches are indeed equivalent, we first write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 

```python
def transform(A,y):
    # First get the vols using vols()
    v0, v1 = vols(A,y)
    # Calculate z vector according to the givern formula
    z = y/v0 + (y-1)/v1
    # Return the result
    return z
```
{::options parse_block_html="true" /}
<div class="gave-help">
Tranforming a vector consisting $$0$$ and $$1$$ to the desired result can be tricky. However, I've found this relatively straightforward solution using linear algebra. I gave suggestions to some of my classmates, saying that instead of constructing seperate vectors or manually selecting the elements, this might be a more efficient way to handle the transform function!
</div>
{::options parse_block_html="false" /}




Then, we check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 

```python
# Calculate the row sum and transform the result to a diagonal matrix
D = A.sum(axis = 1)*np.identity(200)
# Calculate the z vector for the true labels
z = transform(A,y)
# Calculate the z vector for the fake labels
zr = transform(A,ra)
# We check that N_A(C_0,C_1) calculated this way is the same as we calculated above
np.isclose(2*(z@(D-A)@z)/(z@D@z),normcut(A,y))
```




    True




Note that an intersting identity that comes along with the transformation is that $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 


```python
# We check that \mathbf{z}^T\mathbf{D}\mathbb{1} is effectively 0
np.isclose((z@(D-A)@np.ones(200)),0)
```




    True


Note further that in the above transformation, the orthogonal components of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$ can effectively substitute the original $$\mathbf{z}$$. You might think about this as we can decompose the data into parts anticipated by the clustering, and others that could not be effectively explained. This trick will also accelarate the optimization! A brief construction in Python would look like this:

```python
def orth(u, v):
    # A function that calculates the orthogonal projection
    return (u @ v) / (v @ v) * v

# A all-1 vector of size n
e = np.ones(n) 

# Get the diagonal matrix back to a 1D vector
d = D @ e

def orth_obj(z):
    # The orthogonal error
    z_o = z - orth(z, d)
    # Return the result according to the formula
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


### Minimizing The Loss Function

Finally, we are ready to do the optimization work! As we said in the very beginning, in this initial step, we are going to do it *manually*, meaning we will use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. We will give the minimizing vector a name `z_`. 


```python
# We optimize and get the minimizing vector z_ using minimize function from scipy.optimize
from scipy.optimize import minimize
z_ = minimize(orth_obj,z).x
```

**Note**: there's a cheat going on here! We originally specified that the entries of $$\mathbf{z}$$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

### The Results

To visualize the clustering, remember that only the sign of `z_[i]` actually contains information about the cluster label of data point `i`. Thus, we are going to use one color for points whose label `z_[i] < 0` and another color for points whose labe `z_[i] >= 0`. 

We construct the plot and find the clustering to be correct!


```python
# Plot the clustered graph and the result looks just right!
plt.scatter(X[:,0], X[:,1], c = [z_<0])
```


![jpg](/images
/2/E_0.jpg)



## A Much Faster Way

As promised above, we are going to implement a much faster way to do *spectral clustering* in Python using linear algebra. And the motivation is quite clear. If you run the code above and actually do the minimization on your own decive, you may find that the process is extremely slow. However, if we transform the problem using linear algebra, the solution would be purely *algebraic* and thus much faster. We can do this using eigenvalues and eigenvectors.

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

An immediate solution, or an eigenvector to the matrix, as you may recall from the identity we verified above, is $$\mathbb{1}$$. And we know by the result that it is associated with eigenvalue $$0$$ in the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. As we know the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$ is Positive Semi-Definite, then we know $$0$$ is actually the smallest eigenvalue to the matrix. Thus, what we will need to find is the eigenvector that associates with the *second*-smallest eigenvalue, and this is the $$\mathbf{z}$$ we have desired.


Note the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. We will construct this matrix first and then find the eigenvector corresponding to its second-smallest eigenvalue, and we call it `z_eig`. Let's implement this solution and see how we do!



```python
# Construct the normalized L-Transformed matrix of A
L = np.linalg.inv(D)@(D-A)
# Calculate the eigenvalues and the eigenvectors
eigs, U = np.linalg.eig(L)
# Sort the vectors according to the corresponding eigenvalues
ix = eigs.argsort()
# Get the vector corresponding to the second-smallest eig
z_eig = U[:,ix][:,1]
```


```python
# And plotting the culstered graph yields just the right result!
plt.scatter(X[:,0], X[:,1], c = [z_eig<0])
```

![jpg](/images
/2/F_0.jpg)

In fact, `z_eig` should be proportional to `z_`, although this won't be exact because minimization has limited precision by default. But nevertheless, the clustering result is correct!

#### 

Now, we synthesize results from the previous parts. In particular, we write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`. 


### Let's Wrap Up the Solution in a Single Function

In this part, we are going to wrap up the *faster* solution into a single function, going through the process once again. And note that, this will allow as to analyze the effect of different `epsilons` very efficiently!

So, let's contruct the function `spectral_clustering(X, epsilon)` that does the following: 

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Find the eigenvector associating with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 


```python
def spectral_clustering(X, epsilon):
    """
    A spectral-clustering function that clusters a group of points into two clusters w.r.t. a given distance restriction
    
    Input:
    X -> 2-D array, group of points to be clustered
    epsilon -> int, the distance restirction, points within epsilon**2 distance will be considered similar
    
    Output:
    Array-like labels with 0 represent the point is in Cluster 0 and 1 in Cluster 1
    """
    # Generate the similarity matrix for the group of points X
    A = (pairwise_distances(X, metric='euclidean') < epsilon).astype(int)
    np.fill_diagonal(A,0)
    
    # Calculate the row sums and transform into a diagonal matrix
    D = A.sum(axis = 1)*np.identity(A.shape[0])
    # Construct the normalized L-Transformed matrix of A
    L = np.linalg.inv(D)@(D-A)
    
    # Generate the eigenvalues and eigenvectors of L
    eigs, U = np.linalg.eig(L)
    # Sort the vectors according to the corresponding eigenvalues
    ix = eigs.argsort()
    # Get the vector corresponding to the second-smallest eig
    z_eig = U[:,ix][:,1]
    
    # Return the labels
    return (z_eig < 0).astype(int)
```

We try the wrapped-up function on the dataset we created at the beginning:

```python
# Run the spectral_clustering function on X with epsilon 0.16
spectral_clustering(X,0.16)
```




    array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
           0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
           1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
           1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
           1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
           1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
           0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
           1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
           1, 1])




```python
# This generates the same result as our step-by-step approach above
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.16))
```

![jpg](/images
/2/G_0.jpg)


## How `epsilon` affects the clustering

As promised, we are going to look at how the function would perform when we have more complicated data points and when we vary `epsilon`. In this part, we generate different data sets using `make_moons`. We'll discuss how the function respond to different levels of `noise` and what we will need to do with the `epsilon` if we want to get the right result.


Let's start by increasing the noise in the original dataset, and with a slight increase, we find the original `epsilon` is still a valid threshold!

```python
# We raise n to 1000 and set noise at 0.05 with epsilon remaining at 0.16
# The algorithm is able to classify
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.16))
```

![jpg](/images
/2/H_0.jpg)


Again, lets bring the noise to a higher level, and the `epsilon = 0.16` is still an optimal threshold! 

```python
# With n at 1000, and raise noise to 0.08, with epsilon at 0.16, the clustering still looks fine
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.08, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.16))
```

![jpg](/images
/2/H_1.jpg)

However, if we further increase the noise to $$0.1$$, then we will have trouble if we run the code below, as you will see if you run the code multiple times, you will find sometimes this yields a `singular matrix` linear algebra error. That is to say, when we are increasing the noise in the dataset, the points get rather sparse, and the distance between points actually increase. Thereafter, we have a more sparse similarity matrix $$\mathbf{A}$$, and when it's too sparse, the matrix is no longer full rank and thus non-invertible. 

```python
# Further raising the noise to 0.1, we realize we need to relax the distance restriction to 0.45
# Otherwise, the matrix will get singular. And the result is fine!
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.16))
```

To deal with the problem, we simply increase the `epsilon` threshold to around $$0.25$$. And this will generates a correct clustering!

```python
# Further raising the noise to 0.1, we realize we need to relax the distance restriction to 0.45
# Otherwise, the matrix will get singular. And the result is fine!
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.25))
```

![jpg](/images
/2/H_2.jpg)

However, increasing `epsilon` doesn't come with no cost. When the noise in the original dataset, when we increase `epsilon`, we risk the accuracy of the clustering algorithm. In other words, points that should not be similar to each other are now considered similar, this will confuse the algorithm when it `minizes` the number of *similarity relationships cross clusters*. Note below if we further increase the noise to $$0.15$$, there starts to be some confusion on the marginal points.

```python
# Further raise noise to 0.15, we need to again relax some distance restriction
# This time, the clustering seems to get a bit confused on the marginal points
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.25))
```

![jpg](/images
/2/H_3.jpg)

And if we further raise both `noise` and `epsilon` at the same time, the algorithm will have bigger trouble finding the correct clustering. In other words, we are trading the functionality with its accuracy when the noise is large.

```python
# Further raise noise to 0.18, we need to again relax some distance restriction
# This time, the clustering doesn't work very well because the restriction is too loose
X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.18, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.27))
```

![jpg](/images
/2/H_4.jpg)

{::options parse_block_html="true" /}
<div class="got-help">
I got feedback from one of my classmates, suggesting that I should include more discussions of specific impact of different `epsilon` on the algorithm. I totally agree! And I've demonstrated above why we will need to increase the threshold, and what are some disappointing outcomes that come with the increase.
</div>
{::options parse_block_html="false" /}

## Another Dataset

Now let's try the spectral clustering function on another data set -- the bull's eye! 


```python
# We generate a bullt's eye scatter plots with 1000 samples and noise at 0.05
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

![jpg](/images
/2/I_0.jpg)


There are two concentric circles. As before k-means will not do well here at all. 


```python
# The k-means method like before didn't perform well
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![jpg](/images
/2/I_1.jpg)


Note that the *spectral clustering* function can successfully separate the two circles, but we have to provide it with the right `epsilon`. This algorithm will work for `epsilon` up to `0.52`.


```python
# If we use a spectral_clustering with epsilon around 0.52, the result looks correct
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.52))
```

![jpg](/images
/2/I_2.jpg)

Now, if we increase the `epsilon`  by a little bit (0.01), the result is very different. Now, the clustering function would fail.


```python
# However, relax the restriction further will lead to mistake
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.53))
```

![jpg](/images
/2/I_4.jpg)

