---
layout: post
title: Blog Post 3- Identify Fakes News
---

In this blog post, we are going to use machine learning techniques to build a model that identifies fake news online. We will be breaking the modeling into different parts and walk you through all the necessary steps to achieve the goal:

- Extract the training data
- Modify the training data into a proper dataset
- Build and fit the models
- Evaluate the performance of the models
- Review what features (key words) our models have identified

Below are the packages we will be using when constructing and analyzing our model. The function of each package will be discussed individually when we use them in the following parts.


```python
import numpy as np
import pandas as pd
import tensorflow as tf

import re
import string

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from matplotlib import pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

from sklearn.decomposition import PCA
import plotly.express as px 
```

## Acquire Training Data

We first retrieve the dataset. The dataset we use is hosted by Professor Chodrow at UCLA, who is my instructor for PIC 16B. To retrieve the data, we simply use `pd.read_csv()` to read the online file directly.

Note that each row is an individual instance of `news`, which includes the **title** and the **text** of the news. The value in the **fake** column tells us if the piece of news is fake news, where `fake = 1` indicates the news is fake, while `fake = 0` means the news is true.


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```


```python
df = pd.read_csv(train_url)
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>



## Make A Dataset

For this particular dataset, before we train our model on it, we need to note there is some extra preparation work we need to do:

- There are non-important words in the texts and titles. For example, the word `and` will have little significance in telling us about the truthfulness of the news. These words are known as the `stopwords`, which we need to exclude from the dataset.
- In order to pass the dataset to tensorflow (the machine learning package we use), we will transform the dataframe into a `dataset` object that can be correctly handled by tensorflow.

Let's deal with these two tasks one by one

It turns out that there's a standardized library of stopwords which we can simply import for our own use. This stopword list is from the `sklearn.feature_xtraction` module.


```python
from sklearn.feature_extraction import text
stop = text.ENGLISH_STOP_WORDS
```

Now the object `stop` contains the list of stopwords, we then remove all stopwords from our dataframe simply using the `apply` method associated the dataframe object.


```python
df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop)]))
df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop)]))
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result Austria's FPO 'big chall...</td>
      <td>German Chancellor Angela Merkel said Monday st...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER “Close Confidant” Ja...</td>
      <td>December 5, 2017, Circa s Sara Carter warned m...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp offered help Argentina disappeare...</td>
      <td>Germany s Thyssenkrupp, offered assistance Arg...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision travel ban 'p...</td>
      <td>President Donald Trump Thursday called appella...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses Release Clinton-Lynch Ta...</td>
      <td>Clinton Lynch just talked grandkids secret tra...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Pence's vow sling mud survive Trump campaign?</td>
      <td>() - 1990, close bitter congressional race, Mi...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try Spin Way ‘I Love War’...</td>
      <td>new ad Hillary Clinton SuperPac Priorities USA...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates 100 days president, blasts media</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT DEBATE: “Clinton News N...</td>
      <td>MELBOURNE, FL town population 76,000. Trump he...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>



And we will convert the dataframe into a `tf.dataset` object that can be handled by tensorflow in the following way. Note that we might need to recycle the function later, we will wrap it up in a function called `make_dataset()`.


```python
def make_dataset(df):
    """
    A function that creates a fakenews tf.data.Dataset from a dataframe after excluding all stopwords
    It will also add a batch configue to the returned dataset, the size of batches being 100
    
    input: df, pd.DataFrame object
    output: tf.data.Dataset with batch_size 100
    """
    # Excluding the stopwords from the dataframe
    stop = text.ENGLISH_STOP_WORDS
    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop)]))
    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in (stop)]))
    
    # Convert the dataframe into the Dataset object in tensorflow
    # Here, we need to tell tensorflow which columns are the inputs ("text", "title")
    # and which column is the desired out ("fake")
    data = tf.data.Dataset.from_tensor_slices(
            (
                {
                    "title" : df[["title"]],
                    "text" : df[["text"]]            
                }, 
                {
                    "fake" : df[["fake"]]
                }
            )
        )
    
    # We set the dataset batch size to 100
    # This means when we train our model, it will take one batch each time
    # And will not loop over the entire 20000+ rows every time
    data = data.batch(100)
    return data
```


```python
# We create the dataset using the above defined function
data = make_dataset(df)
```

To better assess the model during the training process, we will do a simple **train-test-split** like we did in PIC16A. The `tf.data.Dataset` has built-in methods that will allow us to do this. We will leave out 20% of the data for validation purpose.

*Note the dataset is shuffled already, we will not do the split randomly*


```python
train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

# The take(n method allows us to take the first n 'rows'
train = data.take(train_size)
# And the skip method allows us to ignore the first n 'rows' and take the rest
val   = data.skip(train_size).take(val_size)

# Check the length of the train and test dataset
len(train), len(val)
```




    (180, 45)



## Making The Models

Now, we are reaedy to make the models! Yes, model**s**. Because we are given both `text` and `title` in the dataframe, we might wonder which one will help us more when we want to identify the **fake** news. So we are going to create three models in this part:

- Model 1 will only use information from the titles;
- Model 2 will only use information from the texts;
- Model 3 will take into consideration both.

And we will be able to see how each model performs.

Before we construct didferent models, there's one last step-- `vectorization`. Because the models will not be able directly comprehend words, what matters is actually the `frequency` of the words in the dataset. To materialize this idea, we need to use the `vectorization` idea we learned in PIC16A. However, this time we have buildtin function from tensorflow that will help as to achieve this.


```python
# We first specify the maximum number of different words we want to consider
# In other words, we are only going to look at the 2000 words which appear most frequently in the texts/titles
size_vocabulary = 2000

# The first step in vectorization is to standardize the texts/titles
# This is to say, we are going to make all words appear in lower cases
# and we will drop all the punctuations
# After we define the function, tensorflow will automatically apply it when necessary
def standardization(input_data):
    """
    A standardization function that converts all letters to lower cases and drop punctuations
    
    input: tf.data.Dataset object
    ouput: the same dataset in lower case with no punctuations
    """
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                        '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 


# Then we can use the tool from tensorflow, which is called a vectorization layer
# To use the tool, all we need to do is to pass the size_vocabulary and the standardization function to it
def make_vectorization_layer():
    vectorize_layer = TextVectorization(
    standardize=standardization, # automatically apply the standardization function
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500)
    return vectorize_layer

# Then we are ready to build the models
```

### Model 1: Let's Only Consider the Titles

The model we are going to build will look like a bunch of **layers** stacked on top of each other. For example, we first pass our dataset to a vectorization layer that vectorizes the strings. Then another layer that does the embedding. And then some other layers to identify the important words and features, etc. And using tensorflow, the process can be really simple.

Let's first create a vectorization layer using the function we defined above. Because we are using only the information from the titles, so we `adapt` the vectorization layer using only the `title` column of the dataset.


```python
vectorize_layer1 = make_vectorization_layer()
vectorize_layer1.adapt(train.map(lambda x, y: x["title"]))
```

For the model to know where to start, we will have to tell is what kind of input it should accept. In this case, it's a one-column single input, and we give this particular `input` the name `title`. Please do note that we are not passing the `title` column to the model yet, and this is just a **promise** that we will pass it something as specified here.


```python
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)
```

What should the model output be? The should have different layers to deal with the input, vectorize it, do the embedding and identify the features/important words and finally produce its prediction. Let just do this using the `layer` module form tensorflow.


```python
# First, let's include the vectorizaion layer we definied above
title_features = vectorize_layer1(title_input)
# Then an embedding layer. We use dimension = 10 for the model
title_features = layers.Embedding(size_vocabulary, 10, name = "embedding_title")(title_features)
# Drop 20% of the indicators to avoid overfitting
title_features = layers.Dropout(0.2)(title_features)
# Let's consolidate the features using GlobalAveragePooling
title_features = layers.GlobalAveragePooling1D()(title_features)
# Drop again to avoid overfitting
title_features = layers.Dropout(0.2)(title_features)
# We pass it to a Dense Layer to do feature identification
title_features = layers.Dense(32, activation='relu')(title_features)
# Output the result, because we are dealing with 2 cases: true and false
# thus, the output whould have exactly two units
title_features = layers.Dense(2, name = "fake")(title_features)
```


```python
# Then we make a model with the specified input and output
model1 = keras.Model(
    inputs = [title_input],
    outputs = title_features
)
```

Let's take a look of the model structure!


```python
model1.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 500)               0         
    _________________________________________________________________
    embedding_title (Embedding)  (None, 500, 10)           20000     
    _________________________________________________________________
    dropout (Dropout)            (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 10)                0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________
    

Then we just need to complie and run the model. For simplicity, we will use the most traditional optimizer `adam` and a standard loss function for `category identification`. The metric `accuracy` means if we are making the right prediction or not.


```python
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model1.fit(train,
            validation_data = val,
            epochs = 15, # we train the model 15 times using 15 different batches from the dataset
            verbose = True)
```

    Epoch 1/15
    

    C:\Users\zhaog\.conda\envs\PIC 16B\lib\site-packages\tensorflow\python\keras\engine\functional.py:588: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      warnings.warn(
    

    180/180 [==============================] - 3s 12ms/step - loss: 0.6921 - accuracy: 0.5194 - val_loss: 0.6905 - val_accuracy: 0.5266
    Epoch 2/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.6870 - accuracy: 0.5513 - val_loss: 0.6772 - val_accuracy: 0.5460
    Epoch 3/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 0.7068 - val_loss: 0.5783 - val_accuracy: 0.8577
    Epoch 4/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.4889 - accuracy: 0.8496 - val_loss: 0.4010 - val_accuracy: 0.8674
    Epoch 5/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.3454 - accuracy: 0.8839 - val_loss: 0.2990 - val_accuracy: 0.8883
    Epoch 6/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.2680 - accuracy: 0.9044 - val_loss: 0.2444 - val_accuracy: 0.9031
    Epoch 7/15
    180/180 [==============================] - 2s 10ms/step - loss: 0.2264 - accuracy: 0.9183 - val_loss: 0.2098 - val_accuracy: 0.9182
    Epoch 8/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.1998 - accuracy: 0.9269 - val_loss: 0.1884 - val_accuracy: 0.9265
    Epoch 9/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.1808 - accuracy: 0.9311 - val_loss: 0.1742 - val_accuracy: 0.9321
    Epoch 10/15
    180/180 [==============================] - 2s 10ms/step - loss: 0.1650 - accuracy: 0.9375 - val_loss: 0.1660 - val_accuracy: 0.9321
    Epoch 11/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.1553 - accuracy: 0.9411 - val_loss: 0.1585 - val_accuracy: 0.9344
    Epoch 12/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.1480 - accuracy: 0.9431 - val_loss: 0.1537 - val_accuracy: 0.9357
    Epoch 13/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.1417 - accuracy: 0.9456 - val_loss: 0.1505 - val_accuracy: 0.9362
    Epoch 14/15
    180/180 [==============================] - 2s 11ms/step - loss: 0.1359 - accuracy: 0.9474 - val_loss: 0.1494 - val_accuracy: 0.9366
    Epoch 15/15
    180/180 [==============================] - 2s 10ms/step - loss: 0.1301 - accuracy: 0.9502 - val_loss: 0.1403 - val_accuracy: 0.9443
    

As we train the model multiple times, we can see the accuracy gradually grows and the validation accuracy is also satisfactory, let's plot the accuracy over time!


```python
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.savefig("model1.jpg")
```


    
![jpg](/images/3/model1.jpg)
    


We can see from the above plot that though the accuracy on the training data will likely to increase, the accuracy on the validation dataset will not. This suggests that we might overfit the data if we do more training on the model. So we will stop here, and a **95%** accuracy on the training dataset with a **94%** accuracy on the validation dataset is not bad. 

### Model 2: Only the Texts?

Then, we will follow the same process to create our second model, but this time, we are only using the `text` part of the dataset.


```python
# Another vectorization layer, and this time feeding it only with the text part
vectorize_layer2 = make_vectorization_layer()
vectorize_layer2.adapt(train.map(lambda x, y: x["text"]))
```


```python
# A single input using only the texts
text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```


```python
# First, let's include the vectorizaion layer we definied above
text_features = vectorize_layer2(text_input)
# First, let's include the vectorizaion layer we definied above
text_features = layers.Embedding(size_vocabulary, 10, name = "embedding_text")(text_features)
# Drop 20% of the indicators to avoid overfitting
text_features = layers.Dropout(0.2)(text_features)
# Let's consolidate the features using GlobalAveragePooling
text_features = layers.GlobalAveragePooling1D()(text_features)
# Drop again to avoid overfitting
text_features = layers.Dropout(0.2)(text_features)
# We pass it to a Dense Layer to do feature identification
text_features = layers.Dense(32, activation='relu')(text_features)
# Output the result, because we are dealing with 2 cases: true and false
text_features = layers.Dense(2, name = "fake")(text_features)
```


```python
# Declare the model
model2 = keras.Model(
    inputs = text_input,
    outputs = text_features
)
```


```python
# Take a look at the model structure
model2.summary()
```

    Model: "model_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    text (InputLayer)            [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization_1 (TextVe (None, 500)               0         
    _________________________________________________________________
    embedding_text (Embedding)   (None, 500, 10)           20000     
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d_1 ( (None, 10)                0         
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________
    


```python
# Compile our model2
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
# Train the model
history = model2.fit(train,
            validation_data = val,
            epochs = 15, 
            verbose = True)
```

    Epoch 1/15
    

    C:\Users\zhaog\.conda\envs\PIC 16B\lib\site-packages\tensorflow\python\keras\engine\functional.py:588: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      warnings.warn(
    

    180/180 [==============================] - 4s 22ms/step - loss: 0.6366 - accuracy: 0.7318 - val_loss: 0.4747 - val_accuracy: 0.9427
    Epoch 2/15
    180/180 [==============================] - 4s 23ms/step - loss: 0.2971 - accuracy: 0.9379 - val_loss: 0.1866 - val_accuracy: 0.9674
    Epoch 3/15
    180/180 [==============================] - 4s 24ms/step - loss: 0.1544 - accuracy: 0.9659 - val_loss: 0.1237 - val_accuracy: 0.9753
    Epoch 4/15
    180/180 [==============================] - 4s 22ms/step - loss: 0.1107 - accuracy: 0.9756 - val_loss: 0.0967 - val_accuracy: 0.9786
    Epoch 5/15
    180/180 [==============================] - 4s 22ms/step - loss: 0.0885 - accuracy: 0.9798 - val_loss: 0.0823 - val_accuracy: 0.98200.0890 - accuracy: 0.97 - ETA: 0s - loss: 0.0886 - ac
    Epoch 6/15
    180/180 [==============================] - 4s 23ms/step - loss: 0.0738 - accuracy: 0.9832 - val_loss: 0.0743 - val_accuracy: 0.9836
    Epoch 7/15
    180/180 [==============================] - 4s 23ms/step - loss: 0.0647 - accuracy: 0.9846 - val_loss: 0.0681 - val_accuracy: 0.9836
    Epoch 8/15
    180/180 [==============================] - 4s 22ms/step - loss: 0.0563 - accuracy: 0.9872 - val_loss: 0.0652 - val_accuracy: 0.9849
    Epoch 9/15
    180/180 [==============================] - 4s 22ms/step - loss: 0.0506 - accuracy: 0.9882 - val_loss: 0.0622 - val_accuracy: 0.9843
    Epoch 10/15
    180/180 [==============================] - 4s 24ms/step - loss: 0.0457 - accuracy: 0.9892 - val_loss: 0.0607 - val_accuracy: 0.9831
    Epoch 11/15
    180/180 [==============================] - 4s 23ms/step - loss: 0.0400 - accuracy: 0.9906 - val_loss: 0.0603 - val_accuracy: 0.9843
    Epoch 12/15
    180/180 [==============================] - 4s 22ms/step - loss: 0.0369 - accuracy: 0.9914 - val_loss: 0.0589 - val_accuracy: 0.9847
    Epoch 13/15
    180/180 [==============================] - 4s 22ms/step - loss: 0.0334 - accuracy: 0.9919 - val_loss: 0.0594 - val_accuracy: 0.9849
    Epoch 14/15
    180/180 [==============================] - 4s 24ms/step - loss: 0.0303 - accuracy: 0.9928 - val_loss: 0.0592 - val_accuracy: 0.9858
    Epoch 15/15
    180/180 [==============================] - 4s 23ms/step - loss: 0.0268 - accuracy: 0.9941 - val_loss: 0.0604 - val_accuracy: 0.9858
    

This time, we seems to have even better accuracy! Let's take a look at the accuracy over time.


```python
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.savefig("model2.jpg")
```


    
![jpg](/images/3/model2.jpg)
    


Again, we can see from the above plot that though the accuracy on the training data will likely to increase, the accuracy on the validation dataset will not. This suggests that we might overfit the data if we do more training on the model. So we will stop here, and a **99.4%** accuracy on the training dataset with a **98.5%** accuracy on the validation dataset really good. 

### Model 3: What If We Consider Both?

In our last model, we will try to incorporate information from both the `text` and the `title` in the dataset, and see what we will get!


```python
# Another vectorization layer, and this time feed it with both titles and texts
vectorize_layer3 = make_vectorization_layer()
vectorize_layer3.adapt(train.map(lambda x, y: x["title"]))
vectorize_layer3.adapt(train.map(lambda x, y: x["text"]))
```

In this model, we are going to construct a model that has two parts, one dealing with the `title` and the other part dealing with the `texts`, we will combine both parts afterwards and make the right prediction.


```python
embedding_layer = layers.Embedding(size_vocabulary, 10, name = "embedding")
```


```python
text_features = vectorize_layer3(text_input)
text_features = embedding_layer(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```


```python
title_features = vectorize_layer3(title_input)
title_features = embedding_layer(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
```

Then, let's combine the two parts using `layers.concatenate` in tensorflow.


```python
main = layers.concatenate([text_features, title_features], axis = 1)
```

We will add a few more layers to the combined part, most noticeably, we add two more layers of dropout to avoid overfitting.


```python
main = layers.Dropout(0.2)(main)
main = layers.Dense(32, activation='relu')(main)
main = layers.Dropout(0.2)(main)
output = layers.Dense(2, name = "fake")(main)
```

The we build our model!


```python
model3 = keras.Model(
    inputs = [text_input, title_input],
    outputs = output
)
```


```python
model3.summary()
```

    Model: "model_3"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization_3 (TextVecto (None, 500)          0           text[0][0]                       
                                                                     title[0][0]                      
    __________________________________________________________________________________________________
    embedding (Embedding)           (None, 500, 10)      20000       text_vectorization_3[0][0]       
                                                                     text_vectorization_3[1][0]       
    __________________________________________________________________________________________________
    dropout_10 (Dropout)            (None, 500, 10)      0           embedding[0][0]                  
    __________________________________________________________________________________________________
    dropout_12 (Dropout)            (None, 500, 10)      0           embedding[1][0]                  
    __________________________________________________________________________________________________
    global_average_pooling1d_4 (Glo (None, 10)           0           dropout_10[0][0]                 
    __________________________________________________________________________________________________
    global_average_pooling1d_5 (Glo (None, 10)           0           dropout_12[0][0]                 
    __________________________________________________________________________________________________
    dropout_11 (Dropout)            (None, 10)           0           global_average_pooling1d_4[0][0] 
    __________________________________________________________________________________________________
    dropout_13 (Dropout)            (None, 10)           0           global_average_pooling1d_5[0][0] 
    __________________________________________________________________________________________________
    dense_5 (Dense)                 (None, 32)           352         dropout_11[0][0]                 
    __________________________________________________________________________________________________
    dense_6 (Dense)                 (None, 32)           352         dropout_13[0][0]                 
    __________________________________________________________________________________________________
    concatenate_1 (Concatenate)     (None, 64)           0           dense_5[0][0]                    
                                                                     dense_6[0][0]                    
    __________________________________________________________________________________________________
    dropout_14 (Dropout)            (None, 64)           0           concatenate_1[0][0]              
    __________________________________________________________________________________________________
    dense_7 (Dense)                 (None, 32)           2080        dropout_14[0][0]                 
    __________________________________________________________________________________________________
    dropout_15 (Dropout)            (None, 32)           0           dense_7[0][0]                    
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            66          dropout_15[0][0]                 
    ==================================================================================================
    Total params: 22,850
    Trainable params: 22,850
    Non-trainable params: 0
    __________________________________________________________________________________________________
    

Let's compile the model and do the training!


```python
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model3.fit(train,
          epochs = 10,
          validation_data = val, 
          verbose = True)
```

    Epoch 1/10
    180/180 [==============================] - 7s 32ms/step - loss: 0.5825 - accuracy: 0.6879 - val_loss: 0.2309 - val_accuracy: 0.9393 0s - loss: 0.6111 - ac
    Epoch 2/10
    180/180 [==============================] - 5s 29ms/step - loss: 0.1547 - accuracy: 0.9500 - val_loss: 0.0940 - val_accuracy: 0.9757
    Epoch 3/10
    180/180 [==============================] - 6s 32ms/step - loss: 0.0914 - accuracy: 0.9728 - val_loss: 0.0725 - val_accuracy: 0.9818
    Epoch 4/10
    180/180 [==============================] - 6s 34ms/step - loss: 0.0707 - accuracy: 0.9783 - val_loss: 0.0640 - val_accuracy: 0.9849
    Epoch 5/10
    180/180 [==============================] - 6s 31ms/step - loss: 0.0562 - accuracy: 0.9836 - val_loss: 0.0568 - val_accuracy: 0.9840
    Epoch 6/10
    180/180 [==============================] - 5s 30ms/step - loss: 0.0497 - accuracy: 0.9858 - val_loss: 0.0552 - val_accuracy: 0.9838 accuracy
    Epoch 7/10
    180/180 [==============================] - 5s 29ms/step - loss: 0.0403 - accuracy: 0.9893 - val_loss: 0.0542 - val_accuracy: 0.9840
    Epoch 8/10
    180/180 [==============================] - 6s 34ms/step - loss: 0.0337 - accuracy: 0.9912 - val_loss: 0.0534 - val_accuracy: 0.9858
    Epoch 9/10
    180/180 [==============================] - 6s 32ms/step - loss: 0.0300 - accuracy: 0.9922 - val_loss: 0.0601 - val_accuracy: 0.9838
    Epoch 10/10
    180/180 [==============================] - 5s 30ms/step - loss: 0.0262 - accuracy: 0.9942 - val_loss: 0.0581 - val_accuracy: 0.9845
    


```python
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.savefig("model3.jpg")
```


    
![jpg](/images/3/model3.jpg)
    


Again, we can see from the above plot that though the accuracy on the training data will likely to increase, the accuracy on the validation dataset will not. This suggests that we might overfit the data if we do more training on the model. So we will stop here, and a **99.4%** accuracy on the training dataset with a **98.5%** accuracy on the validation dataset really good. 

## Evaluating the Results

Among the three models we have constructed above, we see that the second and the thrid model perform relatively better compared to the first one that only considers the `title` information. Let's use another validation dataset on the two models and see how they would perform!

The dataset can be accessed directly from Professor Chodrow's github page.


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
```


```python
de = pd.read_csv(test_url)
```

Again, in order to feed it to the models, we need to make it a dataset. Here, we recycle the `makedata_set` function we wrote above.


```python
test = make_dataset(de)
```

Let's see first how the second model performs. Let the model predict the labels first


```python
pred = model2.predict(test).argmax(axis = 1)
pred
```

    C:\Users\zhaog\.conda\envs\PIC 16B\lib\site-packages\tensorflow\python\keras\engine\functional.py:588: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      warnings.warn(
    




    array([1, 0, 1, ..., 0, 1, 1], dtype=int64)



And then we see what proportion of the labels the model got right.


```python
acc = 1 - (np.abs(de['fake'].values.reshape(1,-1) - pred)).sum()/len(de)
acc
```




    0.9820036527239521



For **Model2**, it's **98.2%** accurate on the new validation dataset. Not bad!

Let's see how **Model3** does!


```python
pred = model3.predict(test).argmax(axis = 1)
pred
```




    array([1, 0, 1, ..., 0, 1, 1], dtype=int64)




```python
acc = 1 - (np.abs(de['fake'].values.reshape(1,-1) - pred)).sum()/len(de)
acc
```




    0.9835181968016393



**Model3** beats **Model2** by a small margin! It scores **98.35%** accuracy on the validation dataset!

## Word Embedding PCA

In this last, part, let's visualize the features **Model3** found. What words are usually associated with fake news and what are with accurate news?

To do so, we will use the `PCA` module, which stands for `principal component analysis`, it can reduces higher dimensional weights into 1 or 2 dimensional weights, which can help us to visualize the results on a 2-D plot.


```python
weights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer3.get_vocabulary()            # get the vocabulary from our data prep for later

# Apply PCA on the high-dimensional weights
pca = PCA(n_components=1)
weights = pca.fit_transform(weights)

# Create a dataframe accordingly
# Using the first weight as x value
# And the second weight as y value
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : np.arange(len(weights))
})
```


```python
# Plot the weights!
fig = px.scatter(embedding_df, 
          x = "x0", 
          y = "x1", 
          size = list(np.ones(len(embedding_df))),
          size_max = 2,
          hover_name = "word")

from plotly.io import write_html
write_html(fig, "PCA.html")
fig.show()
```

{% include PCA.html %}


After reducing all the weights to 1-D values, we see words on the left are usually associated with accurate news, with specific date indicating words like `tuesday`, `friday` on the far right hand. Words on the left side seem associated with fake news, including `GOP`, `FOX` and some ambiguous expressions like `recently`, `apparently`.

It's also interesting that while `obama`, `hillary` are associated with fake news, `obamas` (or possibly `obama's`) is usually associated with the accurate reportings. That might be explained news are more accurate when they don't speak of the **Specific Peron** but **Things** they did.
